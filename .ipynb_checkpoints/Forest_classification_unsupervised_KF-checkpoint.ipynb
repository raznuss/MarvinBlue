{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10067934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, accuracy_score\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "def load_tif_files(directory):\n",
    "    \"\"\"\n",
    "    Load and return a list of arrays representing TIFF files from a directory.\n",
    "    Each array will have a shape of (3, h, w) corresponding to the three bands (VV, VH, Incidence Angle).\n",
    "    \"\"\"\n",
    "    tif_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.tif')]\n",
    "    bands_list = []\n",
    "    \n",
    "    for file_path in tif_files:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            bands = src.read([1, 2, 3])  # Load bands 1, 2, and 3 (VV, VH, Incidence Angle)\n",
    "            bands_list.append(bands)\n",
    "    \n",
    "    return bands_list\n",
    "\n",
    "# Directories\n",
    "non_forest_dir = r\"D:\\Development\\RESEARCH\\MarvinBlue\\Data\\non_forest\"\n",
    "forest_dir = r\"D:\\Development\\RESEARCH\\MarvinBlue\\Data\\forest\"\n",
    "\n",
    "# Load all data files\n",
    "non_forest_data = load_tif_files(non_forest_dir)\n",
    "forest_data = load_tif_files(forest_dir)\n",
    "\n",
    "print(f\"Loaded {len(non_forest_data)} non-forest files and {len(forest_data)} forest files.\")\n",
    "\n",
    "# Function to extract random non-NaN pixels\n",
    "def extract_random_pixels(data, num_pixels=10000):\n",
    "    \"\"\"\n",
    "    Extract a specified number of random non-NaN pixels from the data.\n",
    "    \"\"\"\n",
    "    pixels = []\n",
    "\n",
    "    for bands in data:\n",
    "        h, w = bands.shape[1], bands.shape[2]\n",
    "        # Flatten the data for easier access to pixels\n",
    "        flat_pixels = bands.reshape(3, h * w).T  # Shape: (h*w, 3)\n",
    "        valid_pixels = flat_pixels[~np.isnan(flat_pixels).any(axis=1)]  # Filter out NaN pixels\n",
    "        \n",
    "        if valid_pixels.shape[0] >= num_pixels:\n",
    "            # Randomly sample the required number of pixels\n",
    "            sampled_pixels = valid_pixels[np.random.choice(valid_pixels.shape[0], num_pixels, replace=False)]\n",
    "            pixels.append(sampled_pixels)\n",
    "        else:\n",
    "            # If not enough valid pixels, sample all available\n",
    "            pixels.append(valid_pixels)\n",
    "    \n",
    "    return np.vstack(pixels)\n",
    "\n",
    "# Extract 10,000 random pixels from each class (forest and non-forest)\n",
    "non_forest_pixels = extract_random_pixels(non_forest_data, num_pixels=10000)\n",
    "forest_pixels = extract_random_pixels(forest_data, num_pixels=10000)\n",
    "\n",
    "# Combine and label the data\n",
    "X = np.vstack([non_forest_pixels, forest_pixels])\n",
    "y = np.hstack([np.zeros(non_forest_pixels.shape[0]), np.ones(forest_pixels.shape[0])])\n",
    "\n",
    "print(f\"Combined feature matrix shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "\n",
    "# # Normalize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# Step 1: Split the Data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_norm, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Step 2: Train the K-Means Model on the Training Set\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "train_clusters = kmeans.predict(X_train)\n",
    "\n",
    "# Step 3: Validate the Model\n",
    "val_clusters = kmeans.predict(X_val)\n",
    "\n",
    "# Step 4: Test the Model\n",
    "test_clusters = kmeans.predict(X_test)\n",
    "\n",
    "# Evaluation: Compare Clusters to True Labels\n",
    "def evaluate_clustering(y_true, clusters):\n",
    "    # Since K-Means doesn't produce labeled clusters, we use adjusted_rand_score\n",
    "    # to measure the similarity between two clusterings.\n",
    "    ari = adjusted_rand_score(y_true, clusters)\n",
    "    print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "    \n",
    "    # Alternatively, if the clusters are binary, you can check accuracy assuming\n",
    "    # labels are aligned. \n",
    "    accuracy = max(accuracy_score(y_true, clusters), accuracy_score(y_true, 1 - clusters))\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on Training Set\n",
    "print(\"Training Set Evaluation:\")\n",
    "evaluate_clustering(y_train, train_clusters)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "print(\"Validation Set Evaluation:\")\n",
    "evaluate_clustering(y_val, val_clusters)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "print(\"Test Set Evaluation:\")\n",
    "evaluate_clustering(y_test, test_clusters)\n",
    "\n",
    "# Optional: Visualize the test set with PCA and the third band (Incidence Angle)\n",
    "pca = PCA(n_components=2)\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "\n",
    "# Optional: Visualize the test set with PCA and the third band (Incidence Angle)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a scatter plot with binary labels for clusters\n",
    "scatter = plt.scatter(X_test_pca[:, 0], X_test[:, 2], c=test_clusters, cmap='viridis', s=2)\n",
    "\n",
    "# Set the colorbar to only show binary values 0 and 1\n",
    "cbar = plt.colorbar(scatter, ticks=[0, 1])\n",
    "cbar.set_label(\"Cluster\")\n",
    "cbar.set_ticks([0, 1])\n",
    "\n",
    "plt.title(\"K-Means Clustering with PCA Component 1 and Incidence Angle (Test Set)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"Incidence Angle\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21216001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c38d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd035d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242a0b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raznu\\AppData\\Local\\Temp\\ipykernel_5512\\559791699.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(time_series_data)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,110,211) into shape (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5512\\559791699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# Load all non-forest and forest files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mnon_forest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tif_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_forest_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mforest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tif_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforest_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5512\\559791699.py\u001b[0m in \u001b[0;36mload_tif_files\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mtime_series_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_series_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Directories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (3,110,211) into shape (3,)"
     ]
    }
   ],
   "source": [
    "# Visualizing the Metrics\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    axes[idx].bar(metrics_df['Model'], metrics_df[f'{metric} (Non-Forest)'], color='skyblue', label=f'{metric} (Non-Forest)')\n",
    "    axes[idx].bar(metrics_df['Model'], metrics_df[f'{metric} (Forest)'], color='lightgreen', label=f'{metric} (Forest)', bottom=metrics_df[f'{metric} (Non-Forest)'])\n",
    "    axes[idx].set_title(f'{metric} Comparison')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the selected classifier\n",
    "\n",
    "# Assuming svm_classifier is your trained SVM model\n",
    "model_path = r'D:\\Development\\RESEARCH\\MarvinBlue\\Results\\svm_model.pkl'\n",
    "joblib.dump(svm_classifier, model_path)\n",
    "print(f\"SVM model saved to {model_path}\")\n",
    "\n",
    "# Save the fitted scaler to a file\n",
    "scaler_path = r'D:\\Development\\RESEARCH\\MarvinBlue\\Results\\scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
